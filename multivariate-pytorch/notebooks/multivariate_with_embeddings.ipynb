{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process want to be a first try to the MULTIVARIATE LSTM model that can accept multiple variable as input and then return a precision prediction for every variable.\n",
    "\n",
    "Next steps: \n",
    "1. make return only one variable that represent the type of log we are waiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = [\n",
    "    \"userAgent\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change in the loading of the data => loading at realtime\n",
    "import pathlib\n",
    "class UnificatedDataset(Dataset):\n",
    "    def __init__(self, path, filename, tokenizer, embeddings_vector):\n",
    "        self.path = pathlib.Path(path) \n",
    "        self.path = self.path / filename\n",
    "\n",
    "        # check if file exists\n",
    "        if not self.path.exists():\n",
    "            print(\"Cannot load dataset, Path did not exists\")\n",
    "            return\n",
    "\n",
    "        self.infile = open(path)\n",
    "        self.lines = self.infile.readlines()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embeddings_vector = embeddings_vector\n",
    "\n",
    "        self.cache = [None]*len(self.lines)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cache[idx] is None:\n",
    "            tokens = self.tokenizer(self.lines[idx])\n",
    "            \n",
    "            # Retrieve embeddings for tokens\n",
    "            embeddings = self.embeddings_vector.get_vecs_by_token(tokens, lower_case_backup=True)\n",
    "            self.cache[idx] = embeddings \n",
    "            return embeddings\n",
    "        return self.cache[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe '42B' embeddings\n",
    "global_vectors = GloVe(name='42B', dim=300)\n",
    "# Tokenize your text\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "# division btw train/test must be done in the files instead of the data\n",
    "input = \"../../data/prepared/\"\n",
    "test_file = \"logfile_test.ndjson\"\n",
    "train_file = \"logfile_train.ndjson\"\n",
    "\n",
    "unificated_test = UnificatedDataset(\n",
    "    input, test_file,                   # file input and position\n",
    "    tokenizer=tokenizer,                # tokenizer setting\n",
    "    embeddings_vector=global_vectors    # embeddings\n",
    ")\n",
    "unificated_train = UnificatedDataset(\n",
    "    input, train_file, \n",
    "    tokenizer=tokenizer, \n",
    "    embeddings_vector=global_vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(unificated_test)\n",
    "train_loader = DataLoader(unificated_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for Torch running\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, batch in enumerate(train_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "    print(\"Train Loader: \", x_batch.shape, y_batch.shape)\n",
    "    break\n",
    "\n",
    "# check on data loaded\n",
    "for _, batch in enumerate(test_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "    print(\"Test Loader: \", x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost: cross-entropy\n",
    "\n",
    "class LSTMMultiVariate(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, num_layers, l_in_feature):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\t\n",
    "\t\tself.l_in_feature = l_in_feature\n",
    "\t\t\n",
    "\t\t# torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "\t\tself.l0 = nn.Linear(self.l_in_feature, self.input_size)\n",
    "\t\t# torch.nn.LSTM(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, \n",
    "\t\t# dropout=0.0, bidirectional=False, proj_size=0, device=None, dtype=None)\n",
    "\t\tself.l1 = nn.LSTM(self.input_size, self.hidden_size, self.num_layers)\n",
    "\t\t\n",
    "\t\tself.l2 = nn.Linear(self.hidden_size, self.l_in_feature)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# run the first layer\n",
    "\t\tx = self.l0(x)\n",
    "\t\t# run the LSTM layer (l1)\n",
    "\t\tbatch_size = x.size(0)\n",
    "\t\th0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\t\tc0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\t\tx, _ = self.l1(x, (h0, c0))\n",
    "\t\t\n",
    "\t\t# Run the last layer => with or without the ReLU. \n",
    "\t\t# Starting from \"with\"\n",
    "\t\t#x = self.l2(x)\n",
    "\t\tx = torch.relu(self.l2(x))\n",
    "\t\treturn x\n",
    "\n",
    "model = LSTMMultiVariate(len(FEATURE), 128, 4)\n",
    "model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 350\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHEN_TO_SEE_INFO = 99\n",
    "\n",
    "def train_one_epoch():\n",
    "\tmodel.train(True)\n",
    "\tprint(f\"Epoch: {epoch+1}\")\n",
    "\trunning_loss = 0.0\n",
    "\n",
    "\tfor batch_index, batch in tqdm(enumerate(train_loader)):\n",
    "\t\tx_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "\t\toutput = model(x_batch)\n",
    "\t\tloss = loss_function(output, y_batch)\n",
    "\t\trunning_loss += loss.item()\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif batch_index % WHEN_TO_SEE_INFO == 0:\n",
    "\t\t\tavg_loss_across_batches = running_loss/WHEN_TO_SEE_INFO\n",
    "\t\t\tprint(\"Batch {0}, Loss {1:.10f}\".format(\n",
    "\t\t\t\tbatch_index+1,\n",
    "\t\t\t\tavg_loss_across_batches\n",
    "\t\t\t))\n",
    "\t\t\trunning_loss = 0.0\n",
    "def validate_one_epoch():\n",
    "\tmodel.train(False)\n",
    "\trunning_loss = 0.0\n",
    "\n",
    "\tfor batch_index in enumerate(test_loader):\n",
    "\t\tx_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutput = model(x_batch)\n",
    "\t\t\tloss = loss_function(output, y_batch)\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\n",
    "\t\tavg_loss_across_batches = running_loss / len(test_loader)\n",
    "\n",
    "\tprint(\"Val Loss: {0:.10f}\".format(avg_loss_across_batches))\n",
    "\tprint('***************************************************')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\ttrain_one_epoch()\n",
    "\tvalidate_one_epoch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
