{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "> Giovanni Foletto - May 30, 2024\n",
    "\n",
    "In this notebook I will investigate a methods to get information about logs passing them in a *embeddings* layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity of both results are basically the same. The distribution is not so good.\n",
    "\n",
    "Superclose to 80%, pretrain that as benign. Then run it again, since it will concentrate more the data and concentrate the graph. Then check \n",
    "\n",
    "Retry with: `benign`, `undecided`, `malicious`.\n",
    "\n",
    "Try data: `key:value`. Not remove the data.\n",
    "Distribution between 3 variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rising/thesis-da/lib/python3.10/site-packages/polars/_cpu_check.py:232: RuntimeWarning: Missing required CPU features.\n",
      "\n",
      "The following required CPU features were not detected:\n",
      "    ssse3, sse4.1, sse4.2, popcnt\n",
      "Continuing to use this version of Polars on this processor will likely result in a crash.\n",
      "Install the `polars-lts-cpu` package instead of `polars` to run Polars with better compatibility.\n",
      "\n",
      "Hint: If you are on an Apple ARM machine (e.g. M1) this is likely due to running Python under Rosetta.\n",
      "It is recommended to install a native version of Python that does not run under Rosetta x86-64 emulation.\n",
      "\n",
      "If you believe this warning to be a false positive, you can set the `POLARS_SKIP_CPU_CHECK` environment variable to bypass this check.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext as text\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import gc\n",
    "from copy import deepcopy as dc\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "\tdef __init__(self, input_file, tokenizer, vec):\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.vec = vec\n",
    "\t\tself.lines = []\n",
    "\t\tself.embeddings = []\n",
    "\t\twith open(input_file) as of:\n",
    "\t\t\tself.lines = of.readlines()\n",
    "\n",
    "\t\tfor line in self.lines:\n",
    "\t\t\tself.lines.append(self.calculate_embedding(line))\n",
    "\t\t\n",
    "\tdef calculate_embedding(self, input):\n",
    "\t\tself.tokens = self.tokenizer(input)\n",
    "\t\treturn self.vec.get_vecs_by_tokens(self.tokens, lower_case_backup=True)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.lines)\n",
    "\n",
    "\tdef __getitem__(self, i):\n",
    "\t\treturn self.lines[i], self.embeddings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = text.vocab.GloVe(name='6B', dim=50)\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_dataset = LogDataset(\n",
    "\t\"../data/raw/unificated.ndjson\", \n",
    "\ttokenizer,\n",
    "\tvec\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, batch in enumerate(train_loader):\n",
    "  x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "  print(x_batch.shape, y_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1306, -0.1011],\n",
      "         [ 0.6891,  0.3014],\n",
      "         [ 0.0328, -0.2631],\n",
      "         [ 0.0926,  0.3759],\n",
      "         [ 0.5456,  0.0521],\n",
      "         [ 0.1061, -0.0774],\n",
      "         [-0.3428,  0.2210]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TxtEmbedSimplify(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, tokenizer):\n",
    "        super(TxtEmbedSimplify, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.red = nn.Linear(embedding_dim, 2)\n",
    "\n",
    "\t\t\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        embeddings = self.embedding(input_text)\n",
    "        output = self.fc(embeddings)\n",
    "        output = self.red(output)\n",
    "        return output\n",
    "    \n",
    "model = TxtEmbedSimplify(vocab_size=50000, embedding_dim=128)\n",
    "input_text = torch.tensor([[1, 2, 3, 4, 5, 1235, 19999]])  # Assuming the text is represented as a sequence of indices\n",
    "output = model(input_text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: tensor([0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.neigh = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.neigh = NearestNeighbors(n_neighbors=self.k, algorithm='auto', metric='euclidean')\n",
    "        self.neigh.fit(self.X_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        distances, indices = self.neigh.kneighbors(X_test)\n",
    "        y_pred = self.y_train[indices].mode(dim=1)[0]\n",
    "        return y_pred\n",
    "\n",
    "# Example usage\n",
    "X_train = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y_train = torch.tensor([0, 0, 1, 1, 2])\n",
    "\n",
    "X_test = torch.tensor([[2, 3], [6, 7], [11, 12]])\n",
    "\n",
    "knn = KNNClassifier(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Predicted labels:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2163, -0.3068, -0.1451,  ..., -0.5404,  0.1290,  0.8808],\n",
      "         [-0.3963, -0.5435, -0.0127,  ..., -0.6086,  0.2866,  0.3548],\n",
      "         [-0.1151, -0.5401,  0.4744,  ..., -0.1545, -0.3873,  1.0624],\n",
      "         ...,\n",
      "         [-0.2552, -0.5139, -0.2054,  ..., -0.2215, -0.6791,  0.2645],\n",
      "         [ 0.4151,  0.1741, -0.6553,  ...,  0.4056, -0.2988, -0.4242],\n",
      "         [ 0.7307,  0.2165, -0.5562,  ...,  0.4656, -0.6639, -0.2994]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-8.0356e-01, -5.0567e-02, -1.7274e-02,  ..., -1.7778e-01,\n",
      "          -4.0623e-01,  6.8475e-01],\n",
      "         [-9.0496e-01,  3.2348e-01, -2.4660e-01,  ..., -4.0812e-01,\n",
      "          -3.7556e-01,  4.3471e-01],\n",
      "         [-1.3830e+00, -9.5320e-02,  2.8186e-01,  ..., -9.2735e-02,\n",
      "           2.9880e-01,  3.5581e-01],\n",
      "         ...,\n",
      "         [-2.8888e-01, -2.0850e-01, -7.5742e-03,  ..., -2.3136e-01,\n",
      "           3.8235e-01, -9.2748e-01],\n",
      "         [-5.1660e-01,  2.3752e-01, -2.1572e-01,  ..., -1.1691e-01,\n",
      "          -1.2893e-01,  2.0560e-02],\n",
      "         [ 3.3490e-02,  2.8451e-01,  7.5735e-04,  ...,  1.0637e-01,\n",
      "          -3.3073e-01,  2.7184e-01]]])\n"
     ]
    }
   ],
   "source": [
    "# Define the text input\n",
    "text = '{\"userAgent\": \"Boto3/1.9.201 Python/2.7.12 Linux/4.4.0-157-generic Botocore/1.12.201\", \"eventID\": \"40422e90-d6ec-4c33-9ed3-e206107\", \"errorMessage\": \"Request limit exceeded.\", \"userIdentity\": {\"type\": \"IAMUser\", \"principalId\": \"AIDA9BO36HFBHKGJAO9C1\", \"arn\": \"arn:aws:iam::811596193553:user/backup\", \"accountId\": \"811596193553\", \"accessKeyId\": \"ASIARF55FBMFZBXLKDFW\", \"userName\": \"backup\", \"sessionContext\": {\"sessionIssuer\": {}, \"webIdFederationData\": {}, \"attributes\": {\"mfaAuthenticated\": \"false\", \"creationDate\": \"2019-08-21T07:41:25Z\"}}}, \"eventType\": \"AwsApiCall\", \"errorCode\": \"Client.RequestLimitExceeded\", \"sourceIPAddress\": \"5.205.62.253\", \"eventName\": \"RunInstances\", \"eventSource\": \"ec2.amazonaws.com\", \"recipientAccountId\": \"811596193553\", \"requestParameters\": {\"instancesSet\": {\"items\": [{\"imageId\": \"ami-afde8862bc169b8d2\", \"minCount\": 1, \"maxCount\": 10}]}, \"userData\": \"<sensitiveDataRemoved>\", \"instanceType\": \"r4.16xlarge\", \"blockDeviceMapping\": {}, \"monitoring\": {\"enabled\": false}, \"disableApiTermination\":'\n",
    "\n",
    "# Encode the text using the tokenizer\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "\n",
    "# Pass the input through the BERT model to get the embeddings\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]  # The last hidden-state is the pooled output of the BERT model\n",
    "\n",
    "# The embeddings are now stored in last_hidden_states\n",
    "print(last_hidden_states)  # Output: torch.Size([1, 9, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 494, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New (Embedding => KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rising/thesis-da/lib/python3.10/site-packages/polars/_cpu_check.py:232: RuntimeWarning: Missing required CPU features.\n",
      "\n",
      "The following required CPU features were not detected:\n",
      "    ssse3, sse4.1, sse4.2, popcnt\n",
      "Continuing to use this version of Polars on this processor will likely result in a crash.\n",
      "Install the `polars-lts-cpu` package instead of `polars` to run Polars with better compatibility.\n",
      "\n",
      "Hint: If you are on an Apple ARM machine (e.g. M1) this is likely due to running Python under Rosetta.\n",
      "It is recommended to install a native version of Python that does not run under Rosetta x86-64 emulation.\n",
      "\n",
      "If you believe this warning to be a false positive, you can set the `POLARS_SKIP_CPU_CHECK` environment variable to bypass this check.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torchtext as text\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "\n",
    "from numpy import unique, where\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbeddingModel(nn.Module):\n",
    "    def __init__(self, tokenizer, vocab, translation):\n",
    "        super(TextEmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.translation = translation\n",
    "        \n",
    "    def forward(self, input_text):\n",
    "        \n",
    "        input_text = input_text.translate(self.translation)\n",
    "        tokens = self.tokenizer(input_text)\n",
    "        output = vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
    "\n",
    "        return output\n",
    "\n",
    "translation = {\n",
    "    ord(\".\"): \"\",\n",
    "    ord(\"{\"): \"\",\n",
    "    ord(\"}\"): \"\",\n",
    "    ord(\":\"): \"\",\n",
    "    ord(\"/\"): \"\",\n",
    "    ord(\"-\"): \"\",\n",
    "    ord(\"_\"): \"\",\n",
    "    ord(\"\\\"\"): \"\",\n",
    "    ord(\",\"): \"\",\n",
    "    ord(\";\"): \"\",\n",
    "    ord(\"\\n\"): \"\"\n",
    "}\n",
    "\n",
    "vec = text.vocab.GloVe(name='6B', dim=300)\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "model = TextEmbeddingModel(tokenizer=tokenizer, vocab=vec, translation=translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = {\n",
    "    ord(\".\"): \"\",\n",
    "    ord(\"{\"): \"\",\n",
    "    ord(\"}\"): \"\",\n",
    "    ord(\":\"): \"\",\n",
    "    ord(\"/\"): \"\",\n",
    "    ord(\"-\"): \"\",\n",
    "    ord(\"_\"): \"\",\n",
    "    ord(\"\\\"\"): \"\",\n",
    "    ord(\",\"): \"\",\n",
    "    ord(\";\"): \"\",\n",
    "    ord(\"\\n\"): \"\"\n",
    "}\n",
    "\n",
    "vec = text.vocab.GloVe(name='6B', dim=300)\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def compute_embeddings(input_text):\n",
    "    \n",
    "    input_text = input_text.translate(translation)\n",
    "    tokens = tokenizer(input_text)\n",
    "    output = vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 13753/150000 [02:11<108:55:21,  2.88s/it]"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "N_SAMPLE = 150000\n",
    "\n",
    "with open(\"../../data/raw/unificated.ndjson\") as of:\n",
    "\tlines = of.readlines()[:N_SAMPLE]\n",
    "\n",
    "\tfor line in tqdm(lines):\n",
    "\t\ttt = compute_embeddings(line)\n",
    "\t\tdataset.append(tt.tolist())\n",
    "\n",
    "df = pl.DataFrame(dataset)\n",
    "\n",
    "df.read_csv(\"../../data/prepared/1500m_embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105283"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_model = DBSCAN(eps=1, min_samples=500) # min distance = 1, min_samples=500\n",
    "dbscan_model.fit([i.detatch().numpy() for i in dataset])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
